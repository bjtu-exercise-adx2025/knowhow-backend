# LangGraph æ¨¡å—ä½¿ç”¨æŒ‡å—

LangGraph æ˜¯ä¸€ä¸ªåŸºäº GPT æ¨¡å‹çš„æ™ºèƒ½æ–‡ç« å†…å®¹åˆ†æå’Œå¤„ç†æ¨¡å—ï¼Œç”¨äºåˆ†æè¯­éŸ³è½¬å½•æ–‡æœ¬ä¸ç°æœ‰æ–‡ç« çš„å…³ç³»ï¼Œå¹¶è‡ªåŠ¨å†³å®šæ˜¯å¦åˆ›å»ºæ–°æ–‡ç« æˆ–æ›´æ–°ç°æœ‰æ–‡ç« ã€‚

## æ ¸å¿ƒåŠŸèƒ½

- ğŸ¯ **æ™ºèƒ½å†…å®¹åˆ†æ**: ä½¿ç”¨GPTæ¨¡å‹åˆ†æè¯­éŸ³è½¬å½•æ–‡æœ¬ä¸ç°æœ‰æ–‡ç« çš„å…³ç³»
- ğŸ“ **è‡ªåŠ¨æ›´æ–°å†³ç­–**: æ™ºèƒ½åˆ¤æ–­æ˜¯æ›´æ–°ç°æœ‰æ–‡ç« è¿˜æ˜¯åˆ›å»ºæ–°æ–‡ç« 
- ğŸ”„ **æ‰¹é‡å¤„ç†**: æ”¯æŒæ‰¹é‡å¤„ç†å¤šä¸ªè½¬å½•æ–‡æœ¬ä¸æ–‡ç« çš„ç»„åˆ
- ğŸ—„ï¸ **æ•°æ®åº“é›†æˆ**: ä¸é¡¹ç›®ç°æœ‰SQLAlchemyæ¨¡å‹æ— ç¼é›†æˆ
- âš™ï¸ **çµæ´»é…ç½®**: æ”¯æŒå¤šç§GPTæ¨¡å‹å’Œå‚æ•°é…ç½®
- ğŸ” **Debugæ”¯æŒ**: å®Œæ•´çš„è°ƒè¯•æ—¥å¿—å’Œé…ç½®ç®¡ç†
- ğŸš¨ **é”™è¯¯å¤„ç†**: è¯¦ç»†çš„é”™è¯¯ç åˆ†ç±»å’Œå¼‚å¸¸å¤„ç†æœºåˆ¶

## åœ¨åç«¯æ¥å£ä¸­çš„é›†æˆä½¿ç”¨

### 1. å¯¼å…¥æ¨¡å—

```python
from langgraph import ArticleProcessorService
```

### 2. åœ¨ Flask è·¯ç”±ä¸­ä½¿ç”¨

#### åŸºæœ¬é›†æˆç¤ºä¾‹ï¼ˆæ¨èåœ¨ app/api/v1/article.py ä¸­æ·»åŠ ï¼‰

```python
from langgraph import ArticleProcessorService
from flask import request, jsonify
from flask_jwt_extended import jwt_required, get_jwt_identity

@article_bp.route('/intelligent-processing', methods=['POST'])
@jwt_required()
def intelligent_article_processing():
    """æ™ºèƒ½æ–‡ç« å¤„ç†æ¥å£"""
    try:
        data = request.get_json()
        
        # è·å–å‚æ•°
        transcript_id = data.get('transcript_id')
        article_ids = data.get('article_ids', [])
        user_id = get_jwt_identity()  # ä»JWTè·å–å½“å‰ç”¨æˆ·ID
        
        # åˆ›å»ºLangGraphæœåŠ¡å®ä¾‹
        service = ArticleProcessorService()
        
        # å¤„ç†å†…å®¹
        result = service.process_transcript_with_articles(
            transcript_id=transcript_id,
            article_ids=article_ids,
            user_id=user_id
        )
        
        if result['success']:
            return jsonify({
                "success": True,
                "message": "æ–‡ç« æ™ºèƒ½å¤„ç†å®Œæˆ",
                "data": result['data']
            }), 200
        else:
            return jsonify({
                "success": False,
                "message": result['error_message'],
                "error_code": result['error_code']
            }), 400
            
    except Exception as e:
        return jsonify({
            "success": False,
            "message": f"æ™ºèƒ½å¤„ç†å¤±è´¥: {str(e)}"
        }), 500
```

#### æ‰¹é‡å¤„ç†æ¥å£ç¤ºä¾‹

```python
@article_bp.route('/batch-intelligent-processing', methods=['POST'])
@jwt_required()
def batch_intelligent_processing():
    """æ‰¹é‡æ™ºèƒ½æ–‡ç« å¤„ç†æ¥å£"""
    try:
        data = request.get_json()
        user_id = get_jwt_identity()
        
        # è·å–æ‰¹é‡å¤„ç†æ•°æ®
        # æ ¼å¼: [{"transcript_id": 1, "article_ids": [1, 2]}, ...]
        transcript_article_pairs = data.get('pairs', [])
        
        # åˆ›å»ºLangGraphæœåŠ¡
        service = ArticleProcessorService()
        
        # æ‰¹é‡å¤„ç†
        result = service.batch_process_transcripts(
            transcript_article_pairs=transcript_article_pairs,
            user_id=user_id
        )
        
        return jsonify(result), 200
        
    except Exception as e:
        return jsonify({
            "success": False,
            "message": f"æ‰¹é‡å¤„ç†å¤±è´¥: {str(e)}"
        }), 500
```

### 3. æœåŠ¡çŠ¶æ€æ£€æŸ¥æ¥å£

```python
@article_bp.route('/langgraph-status', methods=['GET'])
@jwt_required()
def langgraph_service_status():
    """æ£€æŸ¥LangGraphæœåŠ¡çŠ¶æ€"""
    try:
        service = ArticleProcessorService()
        status = service.get_service_status()
        
        return jsonify({
            "success": True,
            "data": status
        }), 200
        
    except Exception as e:
        return jsonify({
            "success": False,
            "message": f"çŠ¶æ€æ£€æŸ¥å¤±è´¥: {str(e)}"
        }), 500
```

### 4. é›†æˆåˆ°ç°æœ‰çš„æ–‡å­—è®°å½•å¤„ç†æµç¨‹

å¯ä»¥åœ¨ç°æœ‰çš„ `POST /api/v1/articles/text-record` æ¥å£ä¸­é›†æˆ LangGraphï¼š

```python
@article_bp.route('/text-record', methods=['POST'])
@jwt_required()
def create_text_record():
    """åˆ›å»ºæ–‡å­—è®°å½•ï¼ˆé›†æˆLangGraphæ™ºèƒ½å¤„ç†ï¼‰"""
    try:
        data = request.get_json()
        user_id = get_jwt_identity()
        
        # åŸæœ‰çš„æ–‡å­—è®°å½•åˆ›å»ºé€»è¾‘...
        
        # å¦‚æœéœ€è¦æ™ºèƒ½å¤„ç†
        if data.get('enable_intelligent_processing', False):
            transcript_id = data.get('transcript_id')
            related_article_ids = data.get('related_articles', [])
            
            if transcript_id:
                # ä½¿ç”¨LangGraphè¿›è¡Œæ™ºèƒ½å¤„ç†
                service = ArticleProcessorService()
                langgraph_result = service.process_transcript_with_articles(
                    transcript_id=transcript_id,
                    article_ids=related_article_ids,
                    user_id=user_id
                )
                
                # åœ¨å“åº”ä¸­åŒ…å«LangGraphå¤„ç†ç»“æœ
                response_data = {
                    # ... åŸæœ‰å“åº”æ•°æ®
                    "intelligent_processing": langgraph_result
                }
                
                return jsonify(response_data), 200
        
        # åŸæœ‰çš„è¿”å›é€»è¾‘...
        
    except Exception as e:
        return jsonify({"error": str(e)}), 500
```

## API è¯·æ±‚å’Œå“åº”æ ¼å¼

### 1. å•ä¸ªè½¬å½•æ–‡æœ¬å¤„ç†

**è¯·æ±‚æ ¼å¼**ï¼š
```json
POST /api/v1/articles/intelligent-processing
Content-Type: application/json
Authorization: Bearer <jwt_token>

{
  "transcript_id": 15,
  "article_ids": [1, 44, 67]
}
```

**æˆåŠŸå“åº”**ï¼š
```json
{
  "success": true,
  "message": "æ–‡ç« æ™ºèƒ½å¤„ç†å®Œæˆ",
  "data": {
    "created_count": 1,
    "updated_count": 2,
    "total_processed": 3,
    "created_articles": [
      {
        "new_id": 123,
        "content_length": 1500
      }
    ],
    "updated_articles": [
      {
        "id": 44,
        "content_length": 2000
      },
      {
        "id": 67,
        "content_length": 1800
      }
    ],
    "analysis_items_count": 3
  }
}
```

**é”™è¯¯å“åº”**ï¼š
```json
{
  "success": false,
  "message": "Transcript with ID 15 not found",
  "error_code": "DB_TRANSCRIPT_NOT_FOUND"
}
```

### 2. æ‰¹é‡å¤„ç†

**è¯·æ±‚æ ¼å¼**ï¼š
```json
POST /api/v1/articles/batch-intelligent-processing
Content-Type: application/json
Authorization: Bearer <jwt_token>

{
  "pairs": [
    {"transcript_id": 15, "article_ids": [1, 44]},
    {"transcript_id": 16, "article_ids": [67, 89, 123]},
    {"transcript_id": 17, "article_ids": []}
  ]
}
```

**å“åº”æ ¼å¼**ï¼š
```json
{
  "success": true,
  "message": "Batch processing completed",
  "overall_stats": {
    "total_pairs": 3,
    "successful_pairs": 2,
    "failed_pairs": 1,
    "total_created": 2,
    "total_updated": 4
  },
  "individual_results": [
    {
      "index": 0,
      "transcript_id": 15,
      "article_ids": [1, 44],
      "result": {
        "success": true,
        "data": {...}
      }
    }
  ]
}
```

## é…ç½®ç®¡ç†

### 1. é»˜è®¤é…ç½®æ–‡ä»¶ (`langgraph/config.json`)

```json
{
  "models": {
    "default": {
      "url": "https://ai-proxy.chatwise.app/openrouter/api/v1",
      "api_key": "***REMOVED_API_KEY***",
      "model_name": "google/gemini-2.5-pro"
    }
  },
  "settings": {
    "timeout": 30,
    "max_retries": 3,
    "temperature": 0.1,
    "max_tokens": 4000
  },
  "debug": {
    "enabled": true,
    "log_level": "DEBUG",
    "log_to_file": true,
    "log_file": "langgraph_debug.log",
    "log_to_console": true,
    "log_requests": true,
    "log_responses": true,
    "log_database_queries": true,
    "log_processing_steps": true
  }
}
```

### 2. åœ¨æ¥å£ä¸­ä½¿ç”¨è‡ªå®šä¹‰é…ç½®

```python
# ä½¿ç”¨ç¯å¢ƒå˜é‡é…ç½®ï¼ˆæ¨èç”¨äºç”Ÿäº§ç¯å¢ƒï¼‰
import os

@article_bp.route('/custom-processing', methods=['POST'])
def custom_processing():
    # åˆ›å»ºä¸´æ—¶é…ç½®
    custom_config = {
        "models": {
            "production": {
                "url": os.getenv('GPT_API_URL'),
                "api_key": os.getenv('GPT_API_KEY'),
                "model_name": os.getenv('GPT_MODEL_NAME', 'gpt-4')
            }
        },
        "settings": {
            "timeout": int(os.getenv('GPT_TIMEOUT', 60)),
            "max_retries": int(os.getenv('GPT_MAX_RETRIES', 3))
        },
        "debug": {
            "enabled": os.getenv('DEBUG', 'false').lower() == 'true'
        }
    }
    
    # å†™å…¥ä¸´æ—¶é…ç½®æ–‡ä»¶
    import json
    import tempfile
    
    with tempfile.NamedTemporaryFile(mode='w', suffix='.json', delete=False) as f:
        json.dump(custom_config, f)
        temp_config_path = f.name
    
    try:
        # ä½¿ç”¨è‡ªå®šä¹‰é…ç½®åˆ›å»ºæœåŠ¡
        service = ArticleProcessorService(
            config_path=temp_config_path,
            model_name="production"
        )
        
        # å¤„ç†é€»è¾‘...
        
    finally:
        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        os.unlink(temp_config_path)
```

### 3. åŠ¨æ€æ›´æ–°æ¨¡å‹é…ç½®

```python
@article_bp.route('/update-model-config', methods=['POST'])
@jwt_required() 
def update_model_configuration():
    """åŠ¨æ€æ›´æ–°GPTæ¨¡å‹é…ç½®"""
    try:
        data = request.get_json()
        
        service = ArticleProcessorService()
        
        result = service.update_model_config(
            model_name=data.get('model_name'),
            url=data.get('url'),
            api_key=data.get('api_key'),
            model_api_name=data.get('model_api_name')
        )
        
        return jsonify(result), 200
        
    except Exception as e:
        return jsonify({
            "success": False,
            "message": str(e)
        }), 500
```

## åœ¨æ¥å£ä¸­çš„é”™è¯¯å¤„ç†

### 1. å¸¸è§é”™è¯¯ç å¤„ç†

```python
@article_bp.route('/robust-processing', methods=['POST'])
@jwt_required()
def robust_processing():
    """å¸¦å®Œæ•´é”™è¯¯å¤„ç†çš„å¤„ç†æ¥å£"""
    try:
        data = request.get_json()
        transcript_id = data.get('transcript_id')
        article_ids = data.get('article_ids', [])
        user_id = get_jwt_identity()
        
        service = ArticleProcessorService()
        result = service.process_transcript_with_articles(
            transcript_id=transcript_id,
            article_ids=article_ids,
            user_id=user_id
        )
        
        if result['success']:
            return jsonify({
                "success": True,
                "message": "å¤„ç†æˆåŠŸ",
                "data": result['data']
            }), 200
        else:
            # æ ¹æ®é”™è¯¯ç è¿”å›ä¸åŒçš„HTTPçŠ¶æ€ç 
            error_code = result.get('error_code')
            
            if error_code == 'DB_TRANSCRIPT_NOT_FOUND':
                return jsonify({
                    "success": False,
                    "message": "è½¬å½•è®°å½•ä¸å­˜åœ¨",
                    "error_code": error_code
                }), 404
            elif error_code == 'DB_ARTICLE_NOT_FOUND':
                return jsonify({
                    "success": False,
                    "message": "æŒ‡å®šçš„æ–‡ç« ä¸å­˜åœ¨",
                    "error_code": error_code
                }), 404
            elif error_code == 'GPT_API_UNAUTHORIZED':
                return jsonify({
                    "success": False,
                    "message": "GPTæœåŠ¡é…ç½®é”™è¯¯",
                    "error_code": error_code
                }), 500
            elif error_code == 'GPT_API_TIMEOUT':
                return jsonify({
                    "success": False,
                    "message": "GPTæœåŠ¡è¶…æ—¶ï¼Œè¯·ç¨åé‡è¯•",
                    "error_code": error_code
                }), 503
            else:
                return jsonify({
                    "success": False,
                    "message": result['error_message'],
                    "error_code": error_code
                }), 400
                
    except Exception as e:
        # è®°å½•æœªé¢„æœŸçš„é”™è¯¯
        import traceback
        traceback.print_exc()
        
        return jsonify({
            "success": False,
            "message": "æœåŠ¡æš‚æ—¶ä¸å¯ç”¨",
            "error_code": "INTERNAL_SERVER_ERROR"
        }), 500
```

### 2. é”™è¯¯ç å¯¹ç…§è¡¨

| é”™è¯¯ç  | æè¿° | HTTPçŠ¶æ€ç  | å¤„ç†å»ºè®® |
|--------|------|------------|----------|
| `DB_CONNECTION_FAILED` | æ•°æ®åº“è¿æ¥å¤±è´¥ | 500 | æ£€æŸ¥æ•°æ®åº“æœåŠ¡çŠ¶æ€ |
| `DB_TRANSCRIPT_NOT_FOUND` | è½¬å½•è®°å½•ä¸å­˜åœ¨ | 404 | éªŒè¯è½¬å½•IDæœ‰æ•ˆæ€§ |
| `DB_ARTICLE_NOT_FOUND` | æ–‡ç« ä¸å­˜åœ¨ | 404 | éªŒè¯æ–‡ç« IDåˆ—è¡¨ |
| `GPT_API_UNAUTHORIZED` | GPT APIè®¤è¯å¤±è´¥ | 500 | æ£€æŸ¥APIå¯†é’¥é…ç½® |
| `GPT_API_TIMEOUT` | GPT APIè¶…æ—¶ | 503 | å¢åŠ è¶…æ—¶æ—¶é—´æˆ–é‡è¯• |
| `GPT_API_QUOTA_EXCEEDED` | APIé…é¢è¶…é™ | 429 | ç­‰å¾…é…é¢é‡ç½®æˆ–å‡çº§ |
| `INVALID_JSON_RESPONSE` | GPTè¿”å›æ ¼å¼æ— æ•ˆ | 500 | æ£€æŸ¥æ¨¡å‹å’Œæç¤ºè¯é…ç½® |
| `PROCESSING_FAILED` | å¤„ç†å¤±è´¥ | 400 | æ£€æŸ¥è¾“å…¥å‚æ•° |

## Flask åº”ç”¨ä¸Šä¸‹æ–‡ç®¡ç†

### 1. ç¡®ä¿åœ¨åº”ç”¨ä¸Šä¸‹æ–‡ä¸­ä½¿ç”¨

```python
from app import create_app
from flask import has_app_context

@article_bp.route('/context-aware-processing', methods=['POST'])
def context_aware_processing():
    """ç¡®ä¿åœ¨æ­£ç¡®çš„åº”ç”¨ä¸Šä¸‹æ–‡ä¸­ä½¿ç”¨LangGraph"""
    
    # LangGraphä¼šè‡ªåŠ¨å¤„ç†åº”ç”¨ä¸Šä¸‹æ–‡ï¼Œä½†ä¹Ÿå¯ä»¥æ‰‹åŠ¨ç¡®ä¿
    if not has_app_context():
        app = create_app()
        with app.app_context():
            service = ArticleProcessorService()
            # å¤„ç†é€»è¾‘...
    else:
        service = ArticleProcessorService()
        # å¤„ç†é€»è¾‘...
```

### 2. å¼‚æ­¥å¤„ç†ï¼ˆæ¨èç”¨äºå¤§é‡æ•°æ®ï¼‰

```python
import asyncio
from concurrent.futures import ThreadPoolExecutor
from app import create_app

def process_in_context(transcript_id, article_ids, user_id):
    """åœ¨Flaskåº”ç”¨ä¸Šä¸‹æ–‡ä¸­å¤„ç†"""
    app = create_app()
    with app.app_context():
        service = ArticleProcessorService()
        return service.process_transcript_with_articles(
            transcript_id, article_ids, user_id
        )

@article_bp.route('/async-processing', methods=['POST'])
@jwt_required()
def async_processing():
    """å¼‚æ­¥å¤„ç†æ¥å£"""
    try:
        data = request.get_json()
        transcript_id = data.get('transcript_id')
        article_ids = data.get('article_ids', [])
        user_id = get_jwt_identity()
        
        # æäº¤åˆ°çº¿ç¨‹æ± å¼‚æ­¥å¤„ç†
        executor = ThreadPoolExecutor(max_workers=1)
        future = executor.submit(
            process_in_context, 
            transcript_id, article_ids, user_id
        )
        
        # å¯ä»¥é€‰æ‹©ç­‰å¾…ç»“æœæˆ–è¿”å›ä»»åŠ¡ID
        # è¿™é‡Œæ¼”ç¤ºç›´æ¥ç­‰å¾…ç»“æœ
        result = future.result(timeout=120)  # 2åˆ†é’Ÿè¶…æ—¶
        
        return jsonify(result), 200
        
    except Exception as e:
        return jsonify({
            "success": False,
            "message": f"å¼‚æ­¥å¤„ç†å¤±è´¥: {str(e)}"
        }), 500
```

## æ€§èƒ½ä¼˜åŒ–å’Œç›‘æ§

### 1. ç¼“å­˜ä¼˜åŒ–

```python
from functools import lru_cache
from flask_caching import Cache

# å‡è®¾å·²é…ç½®Flask-Caching
cache = Cache()

class CachedArticleProcessor:
    def __init__(self):
        self.service = ArticleProcessorService()
    
    @cache.memoize(timeout=300)  # ç¼“å­˜5åˆ†é’Ÿ
    def get_articles_cached(self, article_ids_tuple):
        """ç¼“å­˜æ–‡ç« å†…å®¹"""
        return self.service.db_ops.get_articles_by_ids(list(article_ids_tuple))
    
    def process_with_cache(self, transcript_id, article_ids, user_id):
        # ä½¿ç”¨ç¼“å­˜è·å–æ–‡ç« ï¼Œå‡å°‘æ•°æ®åº“æŸ¥è¯¢
        articles = self.get_articles_cached(tuple(sorted(article_ids)))
        # ç»§ç»­å¤„ç†...

@article_bp.route('/cached-processing', methods=['POST'])
@jwt_required()
def cached_processing():
    """ä½¿ç”¨ç¼“å­˜ä¼˜åŒ–çš„å¤„ç†æ¥å£"""
    processor = CachedArticleProcessor()
    # ä½¿ç”¨ä¼˜åŒ–åçš„å¤„ç†å™¨...
```

### 2. è¯·æ±‚ç›‘æ§å’Œæ—¥å¿—

```python
import time
from app.utils.log_utils import get_logger

logger = get_logger(__name__)

@article_bp.route('/monitored-processing', methods=['POST'])
@jwt_required()
def monitored_processing():
    """å¸¦ç›‘æ§çš„å¤„ç†æ¥å£"""
    start_time = time.time()
    user_id = get_jwt_identity()
    
    try:
        data = request.get_json()
        transcript_id = data.get('transcript_id')
        article_ids = data.get('article_ids', [])
        
        # è®°å½•è¯·æ±‚å¼€å§‹
        logger.info(f"LangGraphå¤„ç†å¼€å§‹ - ç”¨æˆ·: {user_id}, è½¬å½•: {transcript_id}, æ–‡ç« æ•°: {len(article_ids)}")
        
        service = ArticleProcessorService()
        result = service.process_transcript_with_articles(
            transcript_id=transcript_id,
            article_ids=article_ids,
            user_id=user_id
        )
        
        # è®°å½•å¤„ç†ç»“æœå’Œè€—æ—¶
        process_time = time.time() - start_time
        
        if result['success']:
            data_info = result['data']
            logger.info(
                f"LangGraphå¤„ç†æˆåŠŸ - è€—æ—¶: {process_time:.2f}s, "
                f"æ–°å»º: {data_info['created_count']}, æ›´æ–°: {data_info['updated_count']}"
            )
        else:
            logger.error(
                f"LangGraphå¤„ç†å¤±è´¥ - è€—æ—¶: {process_time:.2f}s, "
                f"é”™è¯¯: {result['error_message']}"
            )
        
        return jsonify(result), 200 if result['success'] else 400
        
    except Exception as e:
        process_time = time.time() - start_time
        logger.error(f"LangGraphå¤„ç†å¼‚å¸¸ - è€—æ—¶: {process_time:.2f}s, å¼‚å¸¸: {str(e)}")
        
        return jsonify({
            "success": False,
            "message": "å¤„ç†è¿‡ç¨‹ä¸­å‘ç”Ÿå¼‚å¸¸"
        }), 500
```

## æµ‹è¯•å’Œè°ƒè¯•

### 1. åŸºæœ¬åŠŸèƒ½æµ‹è¯•

```python
# åˆ›å»ºæµ‹è¯•æ¥å£ç”¨äºéªŒè¯åŠŸèƒ½
@article_bp.route('/test-langgraph', methods=['POST'])
@jwt_required()
def test_langgraph():
    """LangGraphåŠŸèƒ½æµ‹è¯•æ¥å£"""
    try:
        # æ£€æŸ¥æœåŠ¡çŠ¶æ€
        service = ArticleProcessorService()
        status = service.get_service_status()
        
        if status['database']['status'] != 'connected':
            return jsonify({
                "success": False,
                "message": "æ•°æ®åº“è¿æ¥å¤±è´¥",
                "status": status
            }), 500
        
        if status['gpt_model']['status'] != 'configured':
            return jsonify({
                "success": False,
                "message": "GPTæ¨¡å‹é…ç½®é”™è¯¯",
                "status": status
            }), 500
        
        # ä½¿ç”¨æµ‹è¯•æ•°æ®
        test_result = service.process_transcript_with_articles(
            transcript_id=15,  # ç¡®ä¿è¿™ä¸ªIDåœ¨æµ‹è¯•æ•°æ®åº“ä¸­å­˜åœ¨
            article_ids=[1, 44],  # ç¡®ä¿è¿™äº›IDåœ¨æµ‹è¯•æ•°æ®åº“ä¸­å­˜åœ¨
            user_id=2
        )
        
        return jsonify({
            "success": True,
            "message": "LangGraphåŠŸèƒ½æµ‹è¯•å®Œæˆ",
            "service_status": status,
            "test_result": test_result
        }), 200
        
    except Exception as e:
        return jsonify({
            "success": False,
            "message": f"æµ‹è¯•å¤±è´¥: {str(e)}"
        }), 500
```

### 2. Debugæ¨¡å¼å¯ç”¨

```python
@article_bp.route('/enable-debug', methods=['POST'])
@jwt_required()
def enable_debug_mode():
    """å¯ç”¨LangGraphè°ƒè¯•æ¨¡å¼"""
    try:
        service = ArticleProcessorService()
        
        # å¯ç”¨è¯¦ç»†è°ƒè¯•
        service.gpt_config.update_debug_config({
            "enabled": True,
            "log_level": "DEBUG",
            "log_to_console": True,
            "log_requests": True,
            "log_responses": True,
            "log_database_queries": True,
            "log_processing_steps": True
        })
        
        # è·å–å½“å‰debugé…ç½®
        debug_config = service.gpt_config.get_debug_config()
        
        return jsonify({
            "success": True,
            "message": "è°ƒè¯•æ¨¡å¼å·²å¯ç”¨",
            "debug_config": debug_config
        }), 200
        
    except Exception as e:
        return jsonify({
            "success": False,
            "message": f"å¯ç”¨è°ƒè¯•æ¨¡å¼å¤±è´¥: {str(e)}"
        }), 500
```

## éƒ¨ç½²å’Œç”Ÿäº§ç¯å¢ƒæ³¨æ„äº‹é¡¹

### 1. ç¯å¢ƒå˜é‡é…ç½®ï¼ˆæ¨èï¼‰

```bash
# åœ¨ç”Ÿäº§ç¯å¢ƒä¸­è®¾ç½®ç¯å¢ƒå˜é‡
export LANGGRAPH_GPT_API_URL="https://api.openai.com/v1"
export LANGGRAPH_GPT_API_KEY="sk-your-production-key"
export LANGGRAPH_GPT_MODEL="gpt-4"
export LANGGRAPH_DEBUG_ENABLED="false"
export LANGGRAPH_LOG_LEVEL="INFO"
```

åœ¨æ¥å£ä¸­ä½¿ç”¨ç¯å¢ƒå˜é‡ï¼š

```python
import os

def create_production_service():
    """åˆ›å»ºç”Ÿäº§ç¯å¢ƒçš„LangGraphæœåŠ¡"""
    config = {
        "models": {
            "production": {
                "url": os.getenv('LANGGRAPH_GPT_API_URL'),
                "api_key": os.getenv('LANGGRAPH_GPT_API_KEY'),
                "model_name": os.getenv('LANGGRAPH_GPT_MODEL', 'gpt-4')
            }
        },
        "settings": {
            "timeout": int(os.getenv('LANGGRAPH_TIMEOUT', '60')),
            "max_retries": int(os.getenv('LANGGRAPH_MAX_RETRIES', '3')),
            "temperature": float(os.getenv('LANGGRAPH_TEMPERATURE', '0.1'))
        },
        "debug": {
            "enabled": os.getenv('LANGGRAPH_DEBUG_ENABLED', 'false').lower() == 'true',
            "log_level": os.getenv('LANGGRAPH_LOG_LEVEL', 'INFO')
        }
    }
    
    # åˆ›å»ºä¸´æ—¶é…ç½®æ–‡ä»¶
    import tempfile
    import json
    
    with tempfile.NamedTemporaryFile(mode='w', suffix='.json', delete=False) as f:
        json.dump(config, f)
        return ArticleProcessorService(config_path=f.name, model_name="production")
```

### 2. é”™è¯¯ç›‘æ§å’Œå‘Šè­¦

```python
# é›†æˆåˆ°ç°æœ‰çš„é”™è¯¯ç›‘æ§ç³»ç»Ÿ
import logging
from app.utils.monitoring import send_alert  # å‡è®¾æœ‰ç›‘æ§ç³»ç»Ÿ

@article_bp.route('/production-processing', methods=['POST'])
@jwt_required()
def production_processing():
    """ç”Ÿäº§ç¯å¢ƒå¤„ç†æ¥å£"""
    try:
        # å¤„ç†é€»è¾‘...
        service = create_production_service()
        result = service.process_transcript_with_articles(...)
        
        # ç›‘æ§å¤±è´¥ç‡
        if not result['success']:
            error_code = result.get('error_code')
            
            # å¯¹å…³é”®é”™è¯¯å‘é€å‘Šè­¦
            if error_code in ['GPT_API_UNAUTHORIZED', 'DB_CONNECTION_FAILED']:
                send_alert(
                    title="LangGraphå…³é”®é”™è¯¯",
                    message=f"é”™è¯¯ç : {error_code}, æ¶ˆæ¯: {result['error_message']}",
                    level="critical"
                )
        
        return jsonify(result)
        
    except Exception as e:
        # è®°å½•åˆ°é”™è¯¯æ—¥å¿—
        logging.error(f"LangGraphå¤„ç†å¼‚å¸¸: {str(e)}", exc_info=True)
        
        # å‘é€å‘Šè­¦
        send_alert(
            title="LangGraphæœªå¤„ç†å¼‚å¸¸",
            message=str(e),
            level="error"
        )
        
        return jsonify({"success": False, "message": "æœåŠ¡å¼‚å¸¸"}), 500
```

### 3. å¹¶å‘å’Œèµ„æºé™åˆ¶

```python
from threading import Semaphore
from functools import wraps

# é™åˆ¶å¹¶å‘GPT APIè°ƒç”¨
langgraph_semaphore = Semaphore(3)  # æœ€å¤š3ä¸ªå¹¶å‘è°ƒç”¨

def limit_langgraph_concurrency(f):
    @wraps(f)
    def decorated_function(*args, **kwargs):
        langgraph_semaphore.acquire()
        try:
            return f(*args, **kwargs)
        finally:
            langgraph_semaphore.release()
    return decorated_function

@article_bp.route('/rate-limited-processing', methods=['POST'])
@jwt_required()
@limit_langgraph_concurrency
def rate_limited_processing():
    """é™åˆ¶å¹¶å‘çš„å¤„ç†æ¥å£"""
    # å¤„ç†é€»è¾‘...
```

## æœ€ä½³å®è·µæ€»ç»“

### 1. å®‰å…¨æ€§
- ğŸ” **APIå¯†é’¥ç®¡ç†**: ä½¿ç”¨ç¯å¢ƒå˜é‡å­˜å‚¨æ•æ„Ÿä¿¡æ¯ï¼Œé¿å…ç¡¬ç¼–ç 
- ğŸ›¡ï¸ **æƒé™éªŒè¯**: æ‰€æœ‰LangGraphæ¥å£éƒ½åº”ä½¿ç”¨JWTè®¤è¯
- ğŸ“ **è¾“å…¥éªŒè¯**: éªŒè¯æ‰€æœ‰ç”¨æˆ·è¾“å…¥ï¼Œé˜²æ­¢æ³¨å…¥æ”»å‡»

### 2. æ€§èƒ½ä¼˜åŒ–
- âš¡ **å¹¶å‘æ§åˆ¶**: é™åˆ¶åŒæ—¶è¿›è¡Œçš„GPT APIè°ƒç”¨æ•°é‡
- ğŸ’¾ **ç¼“å­˜ç­–ç•¥**: å¯¹é¢‘ç¹è®¿é—®çš„æ–‡ç« å†…å®¹ä½¿ç”¨ç¼“å­˜
- ğŸ”„ **å¼‚æ­¥å¤„ç†**: å¯¹äºè€—æ—¶æ“ä½œä½¿ç”¨å¼‚æ­¥å¤„ç†

### 3. ç›‘æ§å’Œæ—¥å¿—
- ğŸ“Š **æ€§èƒ½ç›‘æ§**: è®°å½•å¤„ç†æ—¶é—´å’ŒæˆåŠŸç‡
- ğŸ› **é”™è¯¯è¿½è¸ª**: è¯¦ç»†è®°å½•é”™è¯¯ä¿¡æ¯å’Œå †æ ˆè·Ÿè¸ª
- ğŸ“ˆ **ä¸šåŠ¡æŒ‡æ ‡**: è·Ÿè¸ªæ–‡ç« åˆ›å»º/æ›´æ–°æ•°é‡ç­‰ä¸šåŠ¡æŒ‡æ ‡

### 4. å®¹é”™æ€§
- ğŸ” **é‡è¯•æœºåˆ¶**: å¯¹ä¸´æ—¶å¤±è´¥è¿›è¡Œè‡ªåŠ¨é‡è¯•
- ğŸš¨ **é™çº§ç­–ç•¥**: åœ¨æœåŠ¡ä¸å¯ç”¨æ—¶æä¾›å¤‡é€‰æ–¹æ¡ˆ
- âš ï¸ **å‘Šè­¦æœºåˆ¶**: å¯¹å…³é”®é”™è¯¯åŠæ—¶å‘é€å‘Šè­¦

## æ•…éšœæ’é™¤

### å¸¸è§é—®é¢˜

1. **"æ¨¡å—å¯¼å…¥å¤±è´¥"**
   ```bash
   # ç¡®ä¿é¡¹ç›®æ ¹ç›®å½•åœ¨Pythonè·¯å¾„ä¸­
   export PYTHONPATH="${PYTHONPATH}:/path/to/project2025-backend"
   ```

2. **"æ•°æ®åº“è¿æ¥å¤±è´¥"**
   ```python
   # æµ‹è¯•æ•°æ®åº“è¿æ¥
   service = ArticleProcessorService()
   status = service.get_service_status()
   print(status['database']['status'])
   ```

3. **"GPT APIè°ƒç”¨å¤±è´¥"**
   ```python
   # éªŒè¯APIé…ç½®
   service.gpt_config.validate_model_config("default")
   ```

4. **"Flaskåº”ç”¨ä¸Šä¸‹æ–‡é”™è¯¯"**
   ```python
   # ç¡®ä¿åœ¨åº”ç”¨ä¸Šä¸‹æ–‡ä¸­è°ƒç”¨
   from flask import has_app_context
   if not has_app_context():
       with app.app_context():
           # ä½¿ç”¨LangGraph
   ```

### è°ƒè¯•æ­¥éª¤

1. **å¯ç”¨Debugæ¨¡å¼**: åœ¨é…ç½®ä¸­è®¾ç½® `"debug": {"enabled": true}`
2. **æ£€æŸ¥æ—¥å¿—æ–‡ä»¶**: æŸ¥çœ‹ `langgraph_debug.log` æ–‡ä»¶
3. **éªŒè¯æœåŠ¡çŠ¶æ€**: è°ƒç”¨ `get_service_status()` æ–¹æ³•
4. **æµ‹è¯•å•ä¸ªç»„ä»¶**: åˆ†åˆ«æµ‹è¯•æ•°æ®åº“è¿æ¥å’ŒGPT APIè°ƒç”¨

---

**æŠ€æœ¯æ”¯æŒ**: å¦‚æœ‰é—®é¢˜è¯·è”ç³»å¼€å‘å›¢é˜Ÿæˆ–æŸ¥çœ‹é¡¹ç›®æ–‡æ¡£  
**ç‰ˆæœ¬**: LangGraph v1.0.0  
**æ›´æ–°æ—¥æœŸ**: 2024å¹´